{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaza Journalist Video Classifier - Validation\n",
    "\n",
    "**Multimodal classification with Audio + Vision + OCR**\n",
    "\n",
    "## Instructions:\n",
    "1. **Run Cell 1** - Setup (takes ~5-10 minutes) - Run ONCE\n",
    "2. **Run Cell 2** - Load processing functions - Run ONCE  \n",
    "3. **Run Cell 3** - Upload Excel & Process videos - Run MANY times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"[1/6] Installing Python packages...\"\n",
    "pip install -q yt-dlp pandas openpyxl pytesseract pillow requests\n",
    "\n",
    "echo \"[2/6] Installing system packages...\"\n",
    "apt-get update -qq > /dev/null 2>&1\n",
    "apt-get install -qq tesseract-ocr tesseract-ocr-ara ffmpeg git build-essential\n",
    "\n",
    "echo \"[3/6] Setting up Whisper.cpp...\"\n",
    "if [ ! -d \"whisper.cpp\" ]; then\n",
    "    git clone https://github.com/ggerganov/whisper.cpp.git\n",
    "    cd whisper.cpp && make -j4 && cd ..\n",
    "fi\n",
    "\n",
    "echo \"[4/6] Downloading Whisper model...\"\n",
    "if [ ! -f \"whisper.cpp/models/ggml-base.bin\" ]; then\n",
    "    cd whisper.cpp && bash ./models/download-ggml-model.sh base && cd ..\n",
    "fi\n",
    "\n",
    "echo \"[5/6] Installing Ollama...\"\n",
    "if ! command -v ollama &> /dev/null; then\n",
    "    curl -fsSL https://ollama.com/install.sh | sh\n",
    "fi\n",
    "\n",
    "echo \"[6/6] Starting Ollama and pulling models...\"\n",
    "nohup ollama serve > /tmp/ollama.log 2>&1 &\n",
    "sleep 5\n",
    "\n",
    "echo \"  - Pulling Qwen 2.5 72B...\"\n",
    "ollama pull qwen2.5:72b\n",
    "echo \"  - Pulling LLaVA...\"\n",
    "ollama pull llava-llama-3:8b\n",
    "\n",
    "echo \"\"\n",
    "echo \"✓ Setup complete!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Load Functions (Run Once After Setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All processing functions embedded here\n",
    "import json, subprocess, pandas as pd, tempfile, time, re, os, requests, base64\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "CATEGORIES = [\"Destruction of Property\", \"Displacement\", \"IDF\", \"Jewish Dissent\", \"Inhumane Acts\",\n",
    "              \"Imprisonment\", \"Resilience\", \"Starvation of Civilian\", \"Testimonials\", \"Willful Killing\"]\n",
    "\n",
    "TAGS = [\"Birth Prevention\", \"Call to Action\", \"Ceasefire Violation\", \"Children\", \"Ethnic Cleansing\",\n",
    "        \"Food\", \"Healthcare workers\", \"Hospitals\", \"IDF\", \"Journalists\", \"Media and Journalism\",\n",
    "        \"Other\", \"Repression\", \"Schools\", \"Torture\", \"Water\", \"Women\"]\n",
    "\n",
    "WHISPER_PATH = \"./whisper.cpp/build/bin/whisper-cli\"\n",
    "WHISPER_MODEL = \"./whisper.cpp/models/ggml-base.bin\"\n",
    "LLAVA_MODEL = \"llava-llama-3:8b\"\n",
    "LLM_MODEL = \"qwen2.5:72b\"\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "def download_video(url: str, output: str) -> bool:\n",
    "    try:\n",
    "        r = subprocess.run([\"yt-dlp\", \"-f\", \"best[ext=mp4]\", \"-o\", output, url], capture_output=True, timeout=120)\n",
    "        return r.returncode == 0 and Path(output).exists()\n",
    "    except: return False\n",
    "\n",
    "def extract_audio(video: str, audio: str) -> bool:\n",
    "    try:\n",
    "        r = subprocess.run([\"ffmpeg\", \"-i\", video, \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", \"-y\", audio], capture_output=True, timeout=60)\n",
    "        return r.returncode == 0\n",
    "    except: return False\n",
    "\n",
    "def transcribe_audio(audio: str, lang: str = \"ar\") -> str:\n",
    "    if not os.path.exists(WHISPER_PATH): return \"\"\n",
    "    cmd = [WHISPER_PATH, \"-m\", WHISPER_MODEL, \"-f\", audio, \"-nt\"]\n",
    "    if lang != \"auto\": cmd.extend([\"-l\", lang])\n",
    "    try: return subprocess.run(cmd, capture_output=True, text=True, timeout=120).stdout.strip()\n",
    "    except: return \"\"\n",
    "\n",
    "def extract_frames(video: str, n: int = 5) -> List[str]:\n",
    "    temp = tempfile.mkdtemp()\n",
    "    try:\n",
    "        dur = float(subprocess.run([\"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\", \"-of\", \"default=noprint_wrappers=1:nokey=1\", video],\n",
    "                                   capture_output=True, text=True, timeout=10).stdout.strip())\n",
    "    except: dur = 30.0\n",
    "    frames = []\n",
    "    interval = max(1.0, dur / (n + 1))\n",
    "    for i in range(n):\n",
    "        t = interval * (i + 1)\n",
    "        f = Path(temp) / f\"frame_{i+1:03d}.jpg\"\n",
    "        try:\n",
    "            subprocess.run([\"ffmpeg\", \"-ss\", str(t), \"-i\", video, \"-frames:v\", \"1\", \"-q:v\", \"2\", \"-y\", str(f)], capture_output=True, timeout=10)\n",
    "            if f.exists(): frames.append(str(f))\n",
    "        except: continue\n",
    "    return frames\n",
    "\n",
    "def extract_text(img: str) -> str:\n",
    "    try:\n",
    "        import pytesseract\n",
    "        from PIL import Image\n",
    "        return pytesseract.image_to_string(Image.open(img), lang='ara+eng').strip()\n",
    "    except: return \"\"\n",
    "\n",
    "def analyze_vision(img: str, ctx: str = \"\") -> str:\n",
    "    with open(img, 'rb') as f:\n",
    "        b64 = base64.b64encode(f.read()).decode('utf-8')\n",
    "    prompt = f\"\"\"Analyze this Gaza journalist report frame.\\nContext: {ctx[:500] if ctx else 'No audio'}\\n\\nDescribe:\\n- People (children, women, injured, medical staff, soldiers)\\n- Setting (hospital, tent, destroyed building)\\n- Situation (what's happening)\\n- Evidence (visual indicators)\"\"\"\n",
    "    try:\n",
    "        r = requests.post(OLLAMA_URL, json={\"model\": LLAVA_MODEL, \"prompt\": prompt, \"images\": [b64], \"stream\": False}, timeout=60)\n",
    "        if r.status_code == 200: return r.json().get(\"response\", \"\")\n",
    "    except: pass\n",
    "    return \"\"\n",
    "\n",
    "def classify(transcript: str, ocr: str, vision: List[str]) -> Dict:\n",
    "    sys_prompt = f\"\"\"Analyze Gaza journalist reports using audio, text, and vision.\\n\\n**CATEGORIES**: Deaths→Willful Killing, Food→Starvation, Buildings→Destruction, Tents→Displacement\\n\\n{json.dumps(CATEGORIES, indent=2)}\\n**TAGS**: {json.dumps(TAGS, indent=2)}\\n\\nRespond JSON: {{\"category\": \"name\", \"tags\": [\"tag1\"], \"confidence\": \"high|medium|low\", \"reasoning\": \"...\"}}\"\"\"\n",
    "    content = f\"\"\"{'='*80}\\nMULTIMODAL ANALYSIS\\n{'='*80}\\n\\n1. AUDIO:\\n{transcript or '[No audio]'}\\n\\n2. OCR:\\n{ocr or '[No text]'}\\n\\n3. VISION:\\n{chr(10).join(f'Frame {i+1}: {v}' for i, v in enumerate(vision)) if vision else '[No vision]'}\\n\\n{'='*80}\\nClassify:\"\"\"\n",
    "    try:\n",
    "        r = requests.post(OLLAMA_URL, json={\"model\": LLM_MODEL, \"prompt\": f\"{sys_prompt}\\n\\n{content}\", \"stream\": False, \"format\": \"json\", \"options\": {\"temperature\": 0.1}}, timeout=180)\n",
    "        if r.status_code == 200:\n",
    "            txt = r.json().get(\"response\", \"\").strip()\n",
    "            if txt.startswith(\"```json\"): txt = txt[7:]\n",
    "            elif txt.startswith(\"```\"): txt = txt[3:]\n",
    "            if txt.endswith(\"```\"): txt = txt[:-3]\n",
    "            return json.loads(txt.strip())\n",
    "    except: pass\n",
    "    return {\"category\": \"Unknown\", \"tags\": [], \"confidence\": \"low\", \"reasoning\": \"Error\"}\n",
    "\n",
    "def process_video(video: str, lang: str = \"ar\") -> Dict:\n",
    "    with tempfile.TemporaryDirectory() as tmp:\n",
    "        audio = Path(tmp) / \"audio.wav\"\n",
    "        transcript = transcribe_audio(str(audio), lang) if extract_audio(video, str(audio)) else \"\"\n",
    "        frames = extract_frames(video, 5)\n",
    "        ocr = \"\\n\".join(extract_text(f) for f in frames)\n",
    "        vision = [analyze_vision(f, transcript[:500]) for f in frames[:3]]\n",
    "        result = classify(transcript, ocr, vision)\n",
    "        return {\"transcript_length\": len(transcript), \"ocr_length\": len(ocr), \"vision_frames\": len(vision), **result}\n",
    "\n",
    "def norm_cat(c): return \"Willful Killing\" if str(c).strip() == \"Wilful Killing\" else (str(c).strip() if not pd.isna(c) else \"Unknown\")\n",
    "def norm_tags(t): return [x.strip() for x in re.split(r'[,;]', str(t)) if x.strip()] if not pd.isna(t) else []\n",
    "\n",
    "print(\"✓ Functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Upload Excel & Run Validation (Run Multiple Times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Configuration\n",
    "SAMPLE_SIZE = 30  # Change this number\n",
    "OUTPUT_DIR = \"validation_output\"\n",
    "\n",
    "# Upload Excel\n",
    "print(\"Upload your Excel file:\\n\")\n",
    "uploaded = files.upload()\n",
    "excel = list(uploaded.keys())[0]\n",
    "print(f\"\\n✓ Uploaded: {excel}\\n\")\n",
    "\n",
    "# Read and filter\n",
    "df = pd.read_excel(excel)\n",
    "print(f\"Total entries: {len(df)}\")\n",
    "df['source'] = df['Source Link/URL'].apply(lambda x: 'instagram' if 'instagram' in str(x).lower() else ('twitter' if 'twitter' in str(x).lower() or 'x.com' in str(x).lower() else ('youtube' if 'youtube' in str(x).lower() else ('facebook' if 'facebook' in str(x).lower() else 'other'))))\n",
    "proc = df[df['source'] != 'other']\n",
    "print(f\"Processable: {len(proc)}\\n\")\n",
    "\n",
    "# Sample\n",
    "sample = proc.sample(n=min(SAMPLE_SIZE, len(proc)), random_state=42)\n",
    "\n",
    "# Setup output\n",
    "out = Path(OUTPUT_DIR)\n",
    "out.mkdir(exist_ok=True)\n",
    "vid_dir = out / \"videos\"\n",
    "vid_dir.mkdir(exist_ok=True)\n",
    "\n",
    "results = []\n",
    "print(f\"Processing {len(sample)} videos...\\n\" + \"=\"*80)\n",
    "\n",
    "# Process each video\n",
    "for idx, (i, row) in enumerate(sample.iterrows(), 1):\n",
    "    url = row['Source Link/URL']\n",
    "    human_cat = norm_cat(row['Category'])\n",
    "    human_tags = norm_tags(row['Tags (optional)'])\n",
    "    src = row['source']\n",
    "\n",
    "    print(f\"\\n[{idx}/{len(sample)}] {src.upper()}\")\n",
    "    print(f\"  Human: {human_cat}\")\n",
    "\n",
    "    vpath = vid_dir / f\"video_{i}.mp4\"\n",
    "    if download_video(url, str(vpath)):\n",
    "        print(f\"  [+] Downloaded\")\n",
    "        try:\n",
    "            start = time.time()\n",
    "            cls = process_video(str(vpath))\n",
    "            elapsed = time.time() - start\n",
    "            auto_cat = cls['category']\n",
    "            match = auto_cat == human_cat\n",
    "            print(f\"  [+] Auto: {auto_cat} ({cls['confidence']})\")\n",
    "            print(f\"  {'✓ MATCH' if match else '✗ MISMATCH'} | {elapsed:.1f}s\")\n",
    "            results.append({\"url\": url, \"source\": src, \"human_category\": human_cat, \"human_tags\": human_tags,\n",
    "                          \"automated_category\": auto_cat, \"automated_tags\": cls['tags'], \"category_match\": match,\n",
    "                          \"confidence\": cls['confidence'], \"time\": elapsed, **cls})\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] {str(e)}\")\n",
    "            results.append({\"url\": url, \"source\": src, \"error\": str(e), \"human_category\": human_cat})\n",
    "    else:\n",
    "        print(f\"  [ERROR] Download failed\")\n",
    "        results.append({\"url\": url, \"source\": src, \"error\": \"Download failed\", \"human_category\": human_cat})\n",
    "\n",
    "# Report\n",
    "print(\"\\n\" + \"=\"*80 + \"\\nVALIDATION REPORT\\n\" + \"=\"*80)\n",
    "success = [r for r in results if \"error\" not in r]\n",
    "matches = sum(1 for r in success if r.get(\"category_match\", False))\n",
    "print(f\"\\nProcessed: {len(results)} | Successful: {len(success)} | Failed: {len(results)-len(success)}\")\n",
    "if success:\n",
    "    acc = 100 * matches / len(success)\n",
    "    print(f\"\\nAccuracy: {matches}/{len(success)} ({acc:.1f}%)\")\n",
    "    avg_time = sum(r.get(\"time\", 0) for r in success) / len(success)\n",
    "    print(f\"Avg time: {avg_time:.1f}s/video\")\n",
    "    print(f\"\\nConfidence:\")\n",
    "    for c in [\"high\", \"medium\", \"low\"]:\n",
    "        cnt = sum(1 for r in success if r.get('confidence') == c)\n",
    "        print(f\"  {c.capitalize()}: {cnt} ({100*cnt/len(success):.1f}%)\")\n",
    "\n",
    "# Save\n",
    "with open(out / \"results.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "print(f\"\\n✓ Results: {out}/results.json\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
